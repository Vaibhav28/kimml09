%% This paper describes the results of the Machine Learning practical
%% project. It should be approximately 8 pages long and contain the
%% following sections:
%%
%% 1) Title, authors, abstract
%% 2) Introduction
%% 3) Application
%% 4) Methods
%% 5) Experimental Results
%% 6) Discussion / Conclusion
%% References
\documentclass[preprint,journal,11pt]{vgtc}
\let\ifpdf\relax
%% Uncomment one of the lines above depending on where your paper is
%% in the conference process. ``review'' and ``widereview'' are for review
%% submission, ``preprint'' is for pre-publication, and the final version
%% doesn't use a specific qualifier. Further, ``electronic'' includes
%% hyperreferences for more convenient online viewing.

%% Please use one of the ``review'' options in combination with the
%% assigned online id (see below) ONLY if your paper uses a double blind
%% review process. Some conferences, like IEEE Vis and InfoVis, have NOT
%% in the past.

%% Please note that the use of figures is not permitted on the first page
%% of the journal version.  Figures should begin on the second page and be
%% in CMYK or Grey scale format, otherwise, colour shifting may occur
%% during the printing process.  Papers submitted with figures on the
%% first page will be refused.

%% These three lines bring in essential packages: ``mathptmx'' for Type 1
%% typefaces, ``graphicx'' for inclusion of EPS figures. and ``times''
%% for proper handling of the times font family.

\usepackage{mathptmx}
\usepackage{graphicx,xcolor,booktabs, tabularx, amsmath}
\usepackage{times}
\usepackage{breqn}
\newcommand{\todo}[1]{\textbf{\textcolor{blue}{Todo: #1 }}}
%% We encourage the use of mathptmx for consistent usage of times font
%% throughout the proceedings. However, if you encounter conflicts
%% with other math-related packages, you may want to disable it.

%% If you are submitting a paper to a conference for review with a double
%% blind reviewing process, please replace the value ``0'' below with your
%% OnlineID. Otherwise, you may safely leave it at ``0''.
\onlineid{0}

%% declare the category of your paper, only shown in review mode
\vgtccategory{}

%% allow for this line if you want the electronic option to work properly
%\vgtcinsertpkg

%% In preprint mode you may define your own headline.
\preprinttext{}

%% Paper title.

\title{Comparing Machine Learning algorithms for classifying cognitive states from fMRI data}

%% This is how authors are specified in the journal style

%% indicate IEEE Member or Student Member in form indicated below
\author{Spyros Ioakeimidis, Mattijs Meiboom, Robin Mills, Tijn Schouten}
\authorfooter{
\item
  Mattijs Meiboom is with University of Groningen, E-mail: m.meiboom@student.rug.nl.
\item
  Robin Mills is with University of Groningen, E-mail: r.mills@student.rug.nl.
\item
  Tijn Schouten is with University of Groningen, E-mail: t.m.schouten.2@student.rug.nl.
\item
  Spyros Ioakeimidis  is with University of Groningen, E-mail: s.ioakeimidis@studenr.rug.nl.
}



%other entries to be set up for journal
%\shortauthortitle{}


%% Abstract section.
\abstract{
Functional magnetic resonance imaging (fMRI) is a brain mapping technique that allows researchers to infer activity in a subject's brain. This technique is typically used to determine which areas of the brain are involved in performing a certain task. Using this knowledge, we may also be able to determine what task was performed by looking at fMRI data. In this paper, we present a comparison of the performance of machine learning algorithms in between-subjects classification of cognitive states, using fMRI data. We found that averaging across Regions of Interest (ROIs) gave the best classification performance. All classifiers that we used performed above-chance, and we found that a Multivariate Gaussian classifier outperformed a Naive Bayesian classifier and a Support Vector Machine. Overall we can say that machine learning algorithms provide a feasible method for decoding cognitive states from fMRI data.
} % end of abstract

%% Keywords that describe your work. Will show as 'Index Terms' in journal
%% please capitalize first letter and insert punctuation after last keyword
\keywords{fMRI, Machine learning, Classification.}

%% Copyright space is enabled by default as required by guidelines.
%% It is disabled by the 'review' option or via the following command:
\nocopyrightspace


\begin{document}

%% The ``\maketitle'' command must be the first command after the
%% ``\begin{document}'' command. It prepares and prints the title block.
%% the only exception to this rule is the \firstsection command
\firstsection{Introduction}
\label{sec:introduction}

\maketitle

%% Now the actual contents of first section
Human functional brain mapping is a field of study that has grown strongly in the past decades. This is not a surprise, because it is one of the few ways to look at brain activity of a living animal. There are several techniques available to scan for brain activity in a living human. Of these techniques, functional magnetic resonance imaging (fMRI) has become more and more popular because it has a very good spatial resolution (contrary to MEG/EEG), and it is non-invasive (contrary to PET). Brain activity is inferred from differences in the ratio between oxygenated and deoxygenated blood; when brain activity occurs, more oxygenated blood will flow to a brain region, and this change is detected by a machine. This is called the blood oxygen level dependent (BOLD) response \cite{sm:2012fMRI}.

The fMRI scanner measures the activity in three-dimensional pixels called voxels, which are about 3 x 3 x 3 mm in size, and a brain scan is made somewhere between every 500ms and every few seconds. Before analysing the acquired data, it is usually pre-processed to account for head movements by participants in the scanner and to make between-subjects comparisons possible \cite{sl:2009rl}.

Traditionally, data from each voxel, or from a group of voxels, are analyzed with analysis of variance. Voxels are usually convoluted with a hemodynamic response function in an attempt to infer the spiking neural activity from the slowly building and dropping BOLD response \cite{he:2002na} (cf.~Figure~\ref{fig:bold_response}). In this setup, experimental conditions are the independent variables and a general linear model is fitted to each voxel, or group of voxels. By using this technique, voxels or brain areas can be identified that differ in their average BOLD response across conditions.

Recent studies indicate that this approach may have some serious limitations. One study used a dataset of 1326 subjects, which is much higher than the usual 15 to 25 subjects found in fMRI studies \cite{thy:2012very}. This study explored what happened to a simple contrast between BOLD signal while watching faces and baseline activity, when increasing the number of subjects included in the analysis. They found that, as the number of analyzed subjects increased, an increasing number of brain areas that extended well beyond the expected visual- and face recognition areas showed significant deviation from baseline. Another study, using only three subjects, repeated a simple letter/number discrimination task and visual stimulation. The notable part is that this very simple experiment was run 100 times for each subject, again resulting in exceptionally high signal-to-noise ratio compared to conventional fMRI experiments. In this study the authors again found widespread significant BOLD-responses \cite{go:2012whole}. These findings suggest that significant deviations from baseline activity do not necessarily indicate any special involvement in the performed task, because almost the entire brain will show significant deviations given a high enough signal-to-noise ratio. These findings are perhaps not surprising, because everything we know about the brain indicates that it is best seen as a network with countless connections to other parts of the brain. Therefore, it makes sense to also analyze signals from the brain as a pattern of information coming from the entire brain, rather than attempting to link specific brain areas to experimental conditions.

In fact, Machine learning algorithms have been employed in studies to learn the patterns of activity associated with some experimental condition. Subsequently, these learned patterns can be used on unseen subjects in order to predict experimental conditions from their brain activity patterns. If these predictions are accurate then it means that the identified patterns hold information about the brain activity associated with some experimental task.

The general goal of these machine learning studies is to identify what experimental condition a subject is in at a certain time. The state that a person is in during some trial (e.g., looking at a face) is referred to as \emph{cognitive state}, because the cognitive processes that are associated with being in that state are of interest. Specifically, the aforementioned studies try to infer the cognitive state of a subject from the brain activity that is evoked in a subject by performing that trial. For example, in an experiment where subjects look at a set of pictures (e.g., a star and a plus), they would be in a certain cognitive state that is the result of looking at these pictures. One could then attempt to use machine learning on the fMRI signal that is evoked by the the brain activity associated with these cognitive states, and use this to learn what patterns of activity in the brain are associated with that cognitive state. This \emph{StarPlus} experiment has been performed by researchers where, in addition to viewing pictures, subjects also were tasked to read sentences containing some true or false statement about the pictures. These data have then been used to successfully infer the cognitive states of looking at either sentences or pictures within single subjects \cite{mi:2003within}, and between subjects \cite{wa:2003betw}.

In this study we use the same StarPlus data as in the aforementioned experiment, and use machine learning algorithms to decode the cognitive states of subjects while looking at pictures or while looking at sentences. An interesting difference between many applications of machine learning, and this study is that most machine learning algorithms have the goal of achieving the maximum possible discrimination between conditions or classes. However, for fMRI studies it is important that the pattern of activity that is identified by the algorithm is a pattern associated with the cognitive state. When using non-linear classifiers, it is possible to decode features predictive of the cognitive state, where the brain does not actually respond to these features. For example, when decoding the primary visual cortex with high enough spatial resolution, it would be possible to combine simple features with a non-linear neural network to decode the presence or absence of, for example, faces. However, this information is not actually processed in that part of the brain \cite{to:2012rev}. To avoid this problem it is recommended to only use linear classifiers when decoding cognitive states from fMRI data. The classifiers that we compare in this paper are a Gaussian Naive Bayesian (GNB) classifier, Linear Support Vector Machine (LSVM), and a Multivariate Gaussian (MVG) classifier.

\begin{figure}
	\centering
	\includegraphics[width=60mm]{figures/f1_bold_response}
  	\caption{Above is the hypothetical neural response to some stimulus. Below is the BOLD response that is evoked by the short spike of neural activity. The neural activity shown above is attempted to be inferred from the BOLD response below that is measured by the fMRI scanner. Figure is adopted from Heeger and Ress \cite{he:2002na}.}
  	\label{fig:bold_response}
\end{figure}

The goal of this study is to compare the classification performance of the cognitive states associated with looking at a picture or looking at a sentence between subjects. This objective introduces an important issue, namely, how the data from the fMRI scans can be compared between subjects. There are two main ways this could be achieved. First, groups of voxels that are anatomically defined as belonging to a certain brain region, a so-called region of interest (ROI), can be averaged into a single \emph{supervoxel}, which can be compared between subjects. Second, a talairach coordinate system can be used that maps the voxels of each subject onto an anatomically defined coordinate. The ROI averaging it is a rather crude method, and a lot of information of the individual voxels is lost. However, because the ROIs are relatively easy to identify by an expert they can be compared easily between subjects. By using the Talairach coordinate system the data of each individual voxel can be used~\cite{talairach1988co}. However, because the error of doing this is usually in the order of a centimetre, while the size of the voxels are in the order of millimetres, the coordinates are difficult to generalize. Because we aim to do between-subjects comparisons the generalization performance is important, thus we will use averaged ROIs in order to compare the classifiers \cite{mi:2004coord}.


%In the rest of this paper, we will discuss a number of machine learning techniques and their ability to classify our test data. In section~\ref{sec:methods} we will discuss the setup of our experiments and the analysis performed on our data. After that, in sections~\ref{sec:results} and ~\ref{sec:discussion}, we will present the results from these experiments and discuss them. We conclude this paper with a discussion on possible improvements and suggest some additional experiments that could be done.


\section{Application}
\label{sec:application}

Thus far, machine learning techniques have been successfully applied for decoding the orientation of a stimulus from fMRI scans \cite{ka:2005decoding}, identifying different patterns of activity for animate versus inanimate objects \cite{kr:2008RSA}, and even reconstructing visual input based on information in the fMRI scans \cite{mi:2008re}. The reason that these machine learning techniques work so well is that patterns of voxel activity can be identified rather than a significant deviation from some baseline. Specifically, information can be present in the pattern of voxels that belong to a certain brain region, even when the average signal from that region is zero (i.e., on average it does not differ from the baseline condition) \cite{to:2012rev}.

The studies mentioned before all depend on the question of whether fMRI data carries class information (pattern discrimination) and exploiting this information to perform classification. However, researchers have indicated at least two other basic questions to which classifiers can be applied, being ``where or when is class information represented?'' (pattern localization) and ``how is class information encoded'' or ``how does its encoding relate to known relationships between stimuli'' (pattern characterization)~\cite{pereira2009machine}. While the main focus of this paper is on the application of machine learning techniques in pattern discrimination, we will briefly discuss these two other applications.

Given a classifier that is able to discriminate on a dataset and identify a certain cognitive state, we may ask ourselves which parts of the brain contain the information leading to this identification. \emph{Pattern localization} addresses this problem by selecting the voxels that contribute the strongest and most reliably to the classification. Surprisingly, when taking the intersection of selected voxels across multiple subjects (for example, when performing cross-validation), this intersection is only a fairly small subset of voxels~\cite{pereira2009machine}. The reason for this is that brains differ between subjects in their size, shape, and location of particular cells. As a result, activation patterns often do not line up well. One of the strategies to deal with this is to use the set of voxels active for each subject as seeds in a clustering algorithm to find similar voxels.

In \emph{pattern characterization}, the question of \emph{how} class information is represented takes a central place. While this paper will focus on linear classifiers, some studies in this area resort to a more sophisticated approach and revolve around the question how to correspond relationships in stimulus with those in fMRI data. For example,~\cite{mitchell2008predicting} researches predicting fMRI response for new words based on their similarity to words for which the response is known.

\section{Methods}
\label{sec:methods}

We now continue by discussing the methods applied in our research. The general approach taken for each of the discussed algorithms consists of the following steps:

\begin{enumerate}
	\item Preprocessing
	\item Dimensionality reduction
	\item Training
	\item Testing
\end{enumerate}

Preprocessing was done to transform the dataset into a format suitable for input into the classification algorithm. Dimensionality reduction was required to cope with the high dimensionality of the dataset and may consist of both feature extraction as well as feature selection. Finally, the approach taken to training and testing the classifier deals with preventing overfitting the classifier to training data and verifying accuracy for cross-subject classification. The choices made in each of these steps may affect classifier accuracy, so to obtain a complete understanding of classification accuracies we have tried several methods at each step. In the following sections, these methods will be described in more detail.

The implementation of the algorithms was done using Python combined with modules tailored to mathematics (NumPy, SciPy) and machine learning (ScikitLearn). The original dataset was made available as a MatLab file and we used preprocessing in MatLab to create a number of files containing modified versions of this data, such as normalization and ROI grouping (cf.~Section~\ref{sec:preprocessing}). The source code for the algorithms is available from the authors on request.

\subsection{Data Acquisition and Dimensions}

The dataset used for our experiments was originally collected by Marcel Just et al. from Carnegie Mellon University's Center for Cognitive Brain Imaging and published as the StarPlus fMRI dataset. This set consists of fMRI data for 6 subjects that was obtained while having each subject participate in a number of trials.

In some of these trials (baseline trials), no stimulus was presented and participants were instructed to remain inert. In other trials, subjects were presented with a sequence of two stimuli on a computer screen. The first stimulus was either a picture or a sentence lasting 4 seconds, followed by a blank screen of 4 seconds. After the blank screen, the opposite type of stimulus was presented for 4 seconds, followed by a blank screen of 15 seconds, during which the subject would indicate by pressing a button whether the sentence correctly described the picture. During each trial, approximately 54 fMRI scans have been taken with an interval of 500ms. In total, 14 trials were baseline trials, while half of the remaining trials had a picture as first stimulus and the other half a sentence.

The entire dataset consists of voxel data and meta-data providing information about the dataset. Included in this information is not only the trial and scan each voxel belongs to, but also the type of the trial (picture first or sentence first). This makes the dataset suitable for classification using supervised learning.

In total, we have data available for approximately 4800 voxels per scan, 54 scans per trial, 40 trials per subject (excluding baseline trials) and 6 subjects. Voxels have been grouped into 25 ROIs (cf.~Section~\ref{sec:dimensionalityReduction}).

\subsection{Preprocessing}
\label{sec:preprocessing}

The dataset contains activation values for each voxels with respect to a baseline activation value for a subject, measured before performing the task. However, each subject may respond differently in terms of strength of voxel activation, making it harder to classify across subjects. To counter this, and make optimal use of the information carried by each voxel, we have created normalized copies of the dataset to use as input for the algorithms. The effect of this preprocessing step depends on assumptions made about the form of our data, which are not available. Instead, we had to verify the effect by looking at the resulting classification results (cf.~Table~\ref{tab:results}).

As a first step, we applied row normalization by setting the mean activation value to zero for each trial. The assumption was that the relative activation has more predictive power than the actual value. We then continued by transforming these values to z-scores by setting the standard-deviation to 1. However, this turned out to negatively impact accuracy, indicating that the transformation lost information. For completeness, we also performed normalization of higher-order properties, such as skew and kurtosis, but found a significant further decrease in accuracy. We therefore did not further researched performance on these sets. One other approach we tried was the application of column normalization, setting the mean value across trials to zero. 

\subsection{Dimensionality Reduction}
\label{sec:dimensionalityReduction}
%Useful background info: http://www.brainvoyager.com/bvqx/doc/UsersGuide/WebHelp/Content/MVPATools/MVPA_Basic_Concepts.htm

Typical input for the machine learning algorithms discussed in this paper consists of a number of feature vectors, one for each fMRI scan. Such a feature vector $\vec{x}$ contains measures for the $N$ features we use to represent that scan and is derived from our input data. In the most basic case, we could use the entire set of voxels belonging to a scan, resulting in a feature vector containing potentially (hundreds of) thousands of features.

In the case that we have a very large dimension of features combined with only a limited amount of scans, our classifier may have difficulty learning the right classification function. This problem is formally known as the \emph{Hughes effect}\cite{Hughes1054102} which states that the predictive power of a machine learning algorithm decreases as the dimensionality increases with a fixed number of training examples. To reduce the dimensionality of the input, several different methods were used.

\begin{figure}
	\centering
	\includegraphics[width=0.45\textwidth]{figures/flying_dutchman}
  	\caption{ROIs. These ROIs have been selected as input for the classifiers. From top left to bottom right, descending vertical slices of the brain are depicted. brown = LDLPFC, red = LIPS, green = LOPER, yellow = LTRIA, orange = LT, light blue = LIPL, blue = CALC}
  	\label{fig:fd}
\end{figure}

\textbf{\emph{ROI mapping.}} ROI mapping is a form of feature selection, wherein only a subset of the N-dimensional input space is used. As described in the Section~\ref{sec:introduction}, ROIs are anatomically defined brain areas classified as relevant by an expert in the field. We used 7 of the 25 defined ROIs, labeled  'CALC' 'LIPL' 'LT' 'LTRIA' 'LOPER' 'LIPS' and  'LDLPFC' (cf.~Figure~\ref{fig:fd}). These ROIs have previously been found to be most relevant in discriminating between the cognitive states of looking at sentences or pictures \cite{wa:2003betw}. To further reduce the number of features, average voxel values within these regions are taken as new input features. As with preprocessing, other higher-order features such as the standard deviation, kurtosis, and skewness within ROIs did not increase predictability and were not included in the subsequent analysis. The disadvantage of using ROIs is that they require \emph{a priori} knowledge. When this knowledge is inaccurate, classification accuracy may not be optimal.

\textbf{\emph{Principal Component Analysis (PCA).}} As feature extraction method, we applied PCA. Ideally, the input vectors of the PCA should be the standardized Talairach coordinates, because we are conducting between-subject classification. As this coordinate system appeared to be inaccurate for this dataset, we were forced to use the voxel indices. The problem is that indices of different subjects may not map well onto each other, due to differences in brain size and shape. As a result, the accuracy of the PCA may be greatly reduced.

\textbf{\emph{Temporal dimensionality reduction.}} The BOLD response peaks at approximately 5 seconds after a stimulus is presented (cf.~Figure~\ref{fig:bold_response}). This means that the first scans after a stimulus may not represent activity related to this stimulus and should not be used for classification. To examine which scans provide maximal class information, we plotted the average BOLD activity per scan for all trials of all subjects, given a trial type and ROI. As illustrated in Figure~\ref{fig:boldroi}, scans 10 to 20 provide maximal information for the first stimulus, while scans 23 to 33 provide maximal information for the second stimulus. The other scans were excluded.

\begin{figure}
	\centering
	\includegraphics[width=85mm]{figures/scan_hemodynamic_BOLD_roi}
  	\caption{Haemodynamic response for ROI "CALC". The red line depicts a sentence-picture trial (S-P) and the blue line depicts a picture-sentence trial (P-S). Average BOLD signal for all subjects is plotted against the scan number. The coloured regions depict the mean BOLD signal +/- 1 standard deviation.}
  	\label{fig:boldroi}
\end{figure}

\subsection{Training Method and Classifiers}
\label{sec:trainingMethodsAndClassifiers}

For each classifier, a ``leave one (subject) out" cross validation procedure was used to avoid overfitting. For $N=6$ iterations, $N-1$ subjects functioned as training set, while the remaining subject served as validation set. The classifier accuracy is the average accuracy over all iterations. We compared the following classifiers:\\

\textbf{\emph{Gaussian Naive Bayes (GNB).}} This classifier builds a generative model underlying a class, by estimating the means and the standard deviations of the input features from the training set \cite{bk:2004aplay}. It assumes a Gaussian distribution (cf.~Eq.~\ref{eq:gaussianDistribution}) and independent input features.

\begin{equation}
\label{eq:gaussianDistribution}
p(X|\mu, \sigma) = \frac{1}{\sigma\sqrt{2\pi}}\exp\Big[-\frac{(x - \bar{\mu})^2}{2\sigma^2}\Big]
\end{equation}

where $\mu$ denotes the mean and $\sigma$ the standard deviation. Bayes rule is used to estimate the probability of a class (picture or sentence), given the input features of the validation set (cf.~Eq.~\ref{eq:bayesRule}).

\begin{equation}
\label{eq:bayesRule}
P(C_{i}|X) = \frac{p(X|C_{i})P(C_{i})}{P(X)}
\end{equation}

where $C_i$ denotes the probability of the class (picture or sentence). Two approaches were used to estimate the parameters of the model. In the first method, averages of the selected scans belonging to a stimulus were used to estimate the parameters, while in the second method, the parameters were estimated for each scan separately. In both methods the decision of the classifier is given by Eq.~\ref{eq:maxLikelihood}, which is the maximum likelihood of the two classes over all scans.

\begin{equation}
\label{eq:maxLikelihood}
\operatorname{arg\,max}_i \sum_{t=1}^{N}{\log{P(C_{i}|x^{t})}}
\end{equation}

where $t$ denotes the scan index. The advantage of the first method is that fewer parameters need to be estimated with greater amount of examples. It thus reduces the space and time complexity, and variance of the model. However, because it is likely that the assumptions of equal means and standard deviations for each scan do not hold, the second approach may decrease the inductive bias and enhance the accuracy.

\textbf{\emph{Linear Support Vector Machine (LSVM).}} The LSVM is a discriminant based approach to classification. For an elaborate description see Chapter 10 of \cite{bk:2004aplay}. For each selected scan belonging to a stimulus, the LSVM classifies it as either a picture or a sentence. The final classification of the stimulus is:

\begin{equation}
\label{eq:argmaxsvm}
\operatorname{arg\,max}_i \sum_{t=1}^{N}{b_{i}^{t}}
\end{equation}

where $t$ denotes the scan index and $b$ is given by:

\begin{equation}
b_{i} = \left\{
\begin{array}{l l}
1 \quad \text{if $C_{i}$ is classified}\\
0 \quad \text{otherwise}\\
\end{array} \right.
\end{equation}

\textbf{\emph{Multivariate Gaussian (MVG).}} In MVG, independence between feature vectors is not assumed. Instead, the multivariate distribution is used to estimate the probability of the classes, with the covariance matrix and the means as parameters. The log-probability of a class becomes:

\begin{dmath}
\label{eq:multivariate}
g_{i}(\mathbf{x}) = -\frac{d}{2}\log{2\pi}-\frac{1}{2}\log{|\Sigma_{i}|}-\frac{1}{2}(\mathbf{x}-\mu_{i})^{T}\Sigma_{i}^{-1}(\textbf{x}-\mu_{i})+\log{P(C_{i})}
\end{dmath}

where $g_{i}(\mathbf{x})$ denotes the log-probability of a class, $d$ denotes the dimensionality of $x$ and $\Sigma$ denotes the covariance matrix. The parameters are estimated per scan and the decision method is similar to GNB. The increase in parameters to be estimated compared to GNB (the correlations) may decrease generalization, while also decreasing inductive bias.

\section{Experimental Results}
\label{sec:results}

\begin{table*}[htpb]
\centering
{\small
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Dimensionality Reduction Method} & \textbf{Naive Bayes} & \textbf{Linear SVM} & \textbf{Naive Bayes*} & \textbf{Multivariate*} \\ \hline
PCA & 61.46 &  - & - & - \\
M ROIs & 70.21 & 72.5 & 74.79 & 76.67 \\
Norm-M ROIs & 71.67 & 71.25 & 75.83 & 77.08 \\
Norm-M-SD ROIs & 65.63 & 69.79 & 69.38 & 65.0 \\
Norm/Vox-M ROIs & 73.13 & 76.67 & 76.88 & 79.58 \\
\hline
\end{tabular}}
\caption{Performance results (in \%) of the three classifiers on different methods for feature selection and structure. \emph{M} stands for \emph{Mean}, \emph{Norm} for \emph{Normalisation}, \emph{SD} for \emph{Standard Deviation}, and \emph{Norm/Vox} for \emph{Normalisation per Voxel index}. Columns marked with an asterisk used the total log-likelihood as a decision function (cf.~Section~\ref{sec:trainingMethodsAndClassifiers}).}
\label{tab:results}
\end{table*}

As a comparison measure, we used the performance of the classifiers for predicting the cognitive state that a subject was in. The predictive performance of GNB, LSVM, and MVG was compared. We observed that different feature selection and extraction methods, as well as different structure on the data affects the predictive performance of the classifiers.

Table~\ref{tab:results} illustrates the overall performance of the three classifiers. Rows indicate various feature selection methods. We applied PCA to the original data, while for the rest feature selection methods were applied to the data belonging to the selected ROIs (cf.~Section~\ref{sec:dimensionalityReduction}). For instance, \emph{Norm/Vox-M ROIs} indicates that we normalized over the voxel values of the same voxel index using the mean and standard deviation, while \emph{Norm-M-SD ROIs} indicates that we normalized over the voxel values of the same scan index using the mean and standard deviation. Columns depict the used classifiers. For the classifiers depicted with *, we used a different method for structuring the data, as described in Section~\ref{sec:trainingMethodsAndClassifiers}.

Furthermore, to test the significance of the differences in performance between the different methods we performed a Chi-square test for interesting contrasts, and obtained a confidence interval for their difference in proportions. For the rest of this section we focus only on the case when \emph{Norm/Vox-M ROIs} is used as a feature selection method.

From Table~\ref{tab:results} we can distinguish the fact that when PCA is used for feature extraction, the performance of GNB is lower than when other feature selection methods are used. By using an exact binomial test we found that GNB that used PCA as feature extraction method performed significantly better than chance-level, with a 95\% $CI: 56.9 - 65.8$, $p < 0.001$. However, a Chi-square test for equality of proportions indicated that using PCA as dimensionality reduction method performed significantly worse than a GNB classifier on the means of the selected ROIs, $\chi^2(1) = 7.7848, p < 0.01$. This suggests that maximizing the variance of the data does not maximize information, possibly because the low signal-to-noise ratio of fMRI data does not insure that the principle component with the highest variance also contains a lot of useful information. Additionally, the data may not be well suited to be aligned by voxel index (cf.~Section~\ref{sec:dimensionalityReduction}). The performance listed in Table~\ref{tab:results} is for PCA using 149 components. This value was found by performing classification with a varying amount of components, the results for which are listed in Figure~\ref{fig:pca_components}.

\begin{figure}
	\centering
	\includegraphics[width=90mm]{figures/pca_plot_by_index.png}
  	\caption{Classification accuracy of GNB after performing PCA with an increasing number of components.}
  	\label{fig:pca_components}
\end{figure}

Comparing classifiers when the same data structure is used, we can distinguish the fact that MVG performs better than GNB. However, a Chi-square test did not find this difference to be significant, $\chi^2(1) = 0.8807, p = 0.348$. Furthermore, the difference between LSVM and GNB was also not found to be significant, $\chi^2(1) = 1.4183, p = 0.234$.

Finally, when classifying per scan and combining these classifications into a single classification per trial (cf.~Table~\ref{tab:results} indicated with a *) the performance increases. When looking at the increase in performance for the GNB classifier, we see performance increases from 73.13\% to 76.88\%. However, a Chi-square test did not find this increase in performance to be significant, $\chi^2(1) = 1.6056, p = 0.205$.

\section{Discussion / Conclusion}
\label{sec:discussion}

In this paper we have replicated earlier findings of decoding the cognitive states of looking at sentences or pictures from fMRI data. Specifically, by observing the patterns of activity for a group of subjects, we are able to predict with above-chance accuracy what the cognitive state of an unseen subject is. This shows that machine learning algorithms can be employed to capture useful information from noisy data, and generalize this information between subjects.

To compare our results with other related studies, Wang et al. \cite{wa:2003betw} used GNB and Support Vector Machine (SVM) as cross-subject classifiers in a sentence versus picture study. When using the averaging technique (averaging all voxels in an ROI into a supervoxel) and normalization, GNB yielded a predictive performance of 74.3\%, and SVM 75.3\%. Our results denote a higher performance, where using normalization GNB yielded a performance of 76.67\% and LSVM 76.88\%. 

We found that even though all of our classification procedures yielded well above chance-level classification performance, the difference between the classifiers was seldomly significant. Still, we consistently found that the Multivariate Gaussian (MVG) classifier outperformed the Gaussian Naive Bayes (GNB) classifier and Linear Support Vector Machine (LSVM). This finding suggests that the interactions between different brain areas contain information about the cognitive state, and that these functional connectivity generalizes to an extent to different subjects.

Still, the uncertainty in our predictions are relatively high, with maximum classification performance reaching close to 80\% accuracy for only two classes. The noisy nature of the fMRI data therefore makes it very difficult to draw hard conclusions when decoding a single cognitive state from a brain scan. This is something to be considered before employing fMRI as a diagnostic tool, or as evidence in court.

Overall we can say that machine learning provides a feasible way of extracting useful information from fMRI data. Furthermore, it avoids a strict multiple comparisons correction that is required in null hypothesis testing by using cross-validation. Also, the identification of patterns of activity associated with tasks is encouraged, rather than attempting to identify brain regions that are involved in tasks. In doing this, much can be learned about how information is represented and processed in the brain.

\bibliographystyle{abbrv}
%%use following if all content of bibtex file should be shown
%\nocite{*}
\bibliography{references}
\end{document}