%% This paper describes the results of the Machine Learning practical
%% project. It should be approximately 8 pages long and contain the
%% following sections:
%%
%% 1) Title, authors, abstract
%% 2) Introduction
%% 3) Application
%% 4) Methods
%% 5) Experimental Results
%% 6) Discussion / Conclusion
%% References
\documentclass[preprint,journal,11pt]{vgtc}
\let\ifpdf\relax
%% Uncomment one of the lines above depending on where your paper is
%% in the conference process. ``review'' and ``widereview'' are for review
%% submission, ``preprint'' is for pre-publication, and the final version
%% doesn't use a specific qualifier. Further, ``electronic'' includes
%% hyperreferences for more convenient online viewing.

%% Please use one of the ``review'' options in combination with the
%% assigned online id (see below) ONLY if your paper uses a double blind
%% review process. Some conferences, like IEEE Vis and InfoVis, have NOT
%% in the past.

%% Please note that the use of figures is not permitted on the first page
%% of the journal version.  Figures should begin on the second page and be
%% in CMYK or Grey scale format, otherwise, colour shifting may occur
%% during the printing process.  Papers submitted with figures on the
%% first page will be refused.

%% These three lines bring in essential packages: ``mathptmx'' for Type 1
%% typefaces, ``graphicx'' for inclusion of EPS figures. and ``times''
%% for proper handling of the times font family.

\usepackage{mathptmx}
\usepackage{graphicx,xcolor,booktabs, tabularx,amsmath}
\usepackage{times}
\newcommand{\todo}[1]{\textbf{\textcolor{blue}{Todo: #1 }}}
%% We encourage the use of mathptmx for consistent usage of times font
%% throughout the proceedings. However, if you encounter conflicts
%% with other math-related packages, you may want to disable it.

%% If you are submitting a paper to a conference for review with a double
%% blind reviewing process, please replace the value ``0'' below with your
%% OnlineID. Otherwise, you may safely leave it at ``0''.
\onlineid{0}

%% declare the category of your paper, only shown in review mode
\vgtccategory{}

%% allow for this line if you want the electronic option to work properly
%\vgtcinsertpkg

%% In preprint mode you may define your own headline.
\preprinttext{}

%% Paper title.

\title{Comparing Machine Learning algorithms for classifying cognitive states from fMRI data}

%% This is how authors are specified in the journal style

%% indicate IEEE Member or Student Member in form indicated below
\author{Mattijs Meiboom, Robin Mills, Tijn Schouten, Spyros Ioakeimidis}
\authorfooter{
\item
  Mattijs Meiboom is with University of Groningen, E-mail: m.meiboom@student.rug.nl.
\item
  Robin Mills is with University of Groningen, E-mail: r.mills@student.rug.nl.
\item
  Tijn Schouten is with University of Groningen, E-mail: t.m.schouten.2@student.rug.nl.
\item
  Spyros Ioakeimidis  is with University of Groningen, E-mail: s.ioakeimidis@studenr.rug.nl.
}



%other entries to be set up for journal
%\shortauthortitle{}


%% Abstract section.
\abstract{
Functional magnetic resonance imaging (fMRI) is a brain mapping technique that allows researchers to infer activity in a subject's brain. This technique is typically used to determine which areas of the brain are involved in performing a certain task. Using this knowledge, we may also be able to determine what task was performed by looking at fMRI data. In this paper, we will compare machine learning algorithms in between-subjects classification performance of cognitive states using fMRI data. We found that averaging across ROI gave the best classification performance. All classifiers that we used performed above-chance, and we found that a Multivariate Gaussian classifier outperformed a Naive Bayesian classifier and a Support Vector Machine. Overall we can say that machine learning algorithms provide a feasible method for decoding cognitive states from fMRI data.
} % end of abstract

%% Keywords that describe your work. Will show as 'Index Terms' in journal
%% please capitalize first letter and insert punctuation after last keyword
\keywords{fMRI, Machine learning, Classification.}

%% Copyright space is enabled by default as required by guidelines.
%% It is disabled by the 'review' option or via the following command:
\nocopyrightspace


\begin{document}

%% The ``\maketitle'' command must be the first command after the
%% ``\begin{document}'' command. It prepares and prints the title block.
%% the only exception to this rule is the \firstsection command
\firstsection{Introduction}
\label{sec:introduction}
\maketitle
%% Now the actual contents of first section
Human functional brain mapping is a field of study that has grown strongly in the past decades. This is not a surprise, because it is one of the few ways to look at brain activity of a living animal. There are several techniques available to scan for brain activity in a living human. Of these techniques, functional magnetic resonance imaging (fMRI) has become more and more popular because it has a very good spatial resolution (contrary to MEG/EEG), and it is non-invasive (contrary to PET). Brain activity is inferred from differences in the ratio between oxygenated and deoxygenated blood; when brain activity occurs, more oxygenated blood will flow to a brain region, and this change is detected by the machine. This is called the blood oxygen level dependant (BOLD) response \cite{sm:2012fMRI}.

The fMRI scanner measures the activity in three-dimensional pixels called voxels, which are about 3 x 3 x 3 mm in size, and a brain scan is made somewhere between every 500ms and every few seconds. However, before analysing the data is it usually pre-processed to account for head movements by participants in the scanner, and to make between-subjects comparisons possible \cite{sl:2009rl}.

Traditionally, data from each voxel, or from a group of voxels, are analysed with analysis of variance. Voxels are usually convoluted with a haemodynamic response function in an attempt to infer the spiking neural activity from the slowly building and dropping BOLD response \cite{he:2002na}. Experimental conditions are the independent variables in this setup, and a general linear model is fitted to each voxel, or group of voxels. By using this technique voxels or brain areas can be identified that differ in average BOLD response across conditions.\\
\indent Recent studies indicate that this approach may have some serious limitations. One study used a dataset of 1326 subjects, which is much higher than the usual 15 to 25 subjects found in fMRI studies \cite{thy:2012very}. This study explored what happened to a simple contrast between BOLD signal while watching faces and baseline activity, when increasing the number of subjects included in the analysis. They found that, as the number of analysed subjects increased, an increasing number of brain areas that extended well beyond the expected visual- and face recognition areas showed significant deviation from baseline. Another study, using only three subjects, repeated a simple letter/number discrimination task and visual stimulation. The notable part is that this very simple experiment was run 100 times for each subject, again resulting in exceptionally high signal-to-noise ratio compared to conventional fMRI experiments. In this study the authors again found widespread significant BOLD-responses \cite{go:2012whole}. These findings suggest that significant deviations from baseline activity do not necessarily indicate any special involvement in the performed task, because almost the entire brain will show significant deviations given a high enough signal-to-noise ratio. These findings are perhaps not surprising, because everything we know about the brain indicates that it is best seen as a network with countless connections to other parts of the brain. Therefore, it makes sense to also analyse signals from the brain as a pattern of information coming from the entire brain, rather attempting to link specific brain areas to experimental conditions.

In fact, Machine learning algorithms have been employed to learn the patterns of activity associated with some experimental condition. Subsequently, these learned patterns can be used on unseen subjects in order to predict experimental conditions from their brain activity patterns. When these predictions are accurate then it means that the identified patterns hold information about the brain activity associated with some experimental task.

These machine learning techniques have been successfully used for decoding the orientation of a stimulus from fMRI scans \cite{ka:2005decoding}, identifying different patterns of activity for animate versus inanimate objects \cite{kr:2008RSA}, and even reconstructing visual input based on information in the fMRI scans \cite{mi:2008re}. The reason that these machine learning techniques work so well is that patterns of voxel activity can be identified rather than a significant deviation from some baseline. Specifically, information can be present in the pattern of voxels that belong to a certain brain region, even when the average signal from that region is zero (i.e., on average it does not differ from the baseline condition) \cite{to:2012rev}.
The general goal of these machine learning studies is to identify what experimental condition a subject is in at a certain time. The state that a person is in during some trial (e.g., looking at a face) is referred to as emph{cognitive state}, because the cognitive processes that are associated with being in that state are of interest. Specifically, the aforementioned studies try to infer the cognitive state of a subject from the brain activity that is evoked in a subject by performing that trial. For example, in an experiment where subjects look at a set of pictures (e.g., a star and a plus), they would be in a certain cognitive state that is the result of looking at these pictures. One could then attempt to use machine learning on the fMRI signal that is evoked by the the brain activity associated with these cognitive states, and use this to learn for example what patterns of activity in the brain is associated with that cognitive state. This \emph{StarPlus} experiment has been performed, where in addition to viewing pictures, subjects also read sentences that contained some true or false statement about the pictures. These data have then been used to successfully infer the cognitive states of looking at either sentences or pictures within single subjects \cite{mi:2003within}, and between subjects \cite{wa:2003betw}.

In this study we use the same StarPlus data as in the previously mentioned experiment, and use machine learning algorithms to decode the cognitive states of subjects when they are looking at pictures or when they are looking at sentences. An interesting difference between many applications of machine learning, and this study is that most machine learning algorithms have the goal of achieving the maximum possible discrimination between conditions or classes. However, for fMRI studies it is important that the pattern of activity that is identified by the algorithm is a pattern associated with the cognitive state. When using non-linear classifiers, it is possible to decode features predictive of the cognitive state, where the brain does not actually respond to these features. For example, when decoding the primary visual cortex with high enough spatial resolution, it would be possible to combine simple features with a non-linear neural network to decode the presence or absence of, for example, faces. However, this information is not actually processed in that part of the brain \cite{to:2012rev}. To avoid this problem it is recommended to only use linear classifiers when decoding cognitive states from fMRI data. The classifiers that we compare in this paper are Gaussian Naive Bayesian (GNB) classifiers, Linear Support Vector Machine (LSVM), and a Multivariate Gaussian (MVG) classifier.\\
\indent The goal of this study is to compare the classification performance of the cognitive states associated with looking at a picture or looking at a sentence between subject. This objective introduces an important issue, namely, how the data from the fMRI scans can be compared between subjects. There are two mains ways this could be achieved. First, groups of voxels that are anatomically defined as belonging to a certain brain region, a region of interest (ROI), can be averaged into a single \emph{supervoxel}, which can be compared between subjects. Second, an anatomically defined talairach coordinate system can be used that maps the voxels of each subject onto an anatomically defined coordinate. The ROI averaging it is a rather crude method, and a lot of information of the individual voxels is lost. However, because the ROI's are relatively easy to identify by an expert they can be compared easily between subjects. By using the talairach coordinate system the data of each individual voxel can be used. However, because the error of doing this is usually in the order of a centimetre, while the size of the voxels are in the order of millimetres, the coordinates are difficult to generalise. Because we aim to do between-subjects comparisons the generalisation performance is important, thus we will use averaged ROI's in order to compare the classifiers \cite{mi:2004coord}.\\


%In the rest of this paper, we will discuss a number of machine learning techniques and their ability to classify our test data. In section~\ref{sec:methods} we will discuss the setup of our experiments and the analysis performed on our data. After that, in sections~\ref{sec:results} and ~\ref{sec:discussion}, we will present the results from these experiments and discuss them. We conclude this paper with a discussion on possible improvements and suggest some additional experiments that could be done.


\section{Application}
\label{sec:application}


What can we do with it now?

- classification problem
- prediction
- cognitive state
\section{Methods}
\label{sec:methods}
General approach consisting of three steps, plus basic data acquisition and preprocessing.
\begin{enumerate}
  \item Feature selection
  \item Training
  \item Testing
\end{enumerate}



- comparison
- classifying subtrials vs trials (ie 54 vs 27)
- analysis / tools (python etc)

\subsection{Data acquisition and dimensions}
The dataset used for our experiments was originally collected by Marcel Just et al. from Carnegie Mellon University's Center for Cognitive Brain Imaging and published as the StarPlus fMRI dataset. This set consists of fMRI data for 6 subjects that was obtained while having each subject participate in a number of trials.

In these trials, subjects were presented with a stimulus on a computer screen. This stimulus was either a picture or a sentence. After a period of time, the stimulus would be replaced with a stimulus of the opposite type (i.e. picture with sentence and vice versa) after which the subject would indicate by pressing a button whether the sentence correctly described the picture. At the end of each trial, approximately 54 fMRI scans have be taken with an interval of 500ms.
The entire dataset consist of voxel data and metadata providing information about the dataset. Included in this information is not only the trial and scan each voxel belongs to, but also the type of the trial (picture first or sentence first). This makes the dataset suitable for classification using supervised learning.

In total, we have data available for approximately 4800 voxels per scan, 54 scans per trial, 40 trials per subject, 6 subjects. Voxels have been grouped into 25 regions of interest (see \textit{Feature selection}) that may be of use for analysis.

Here we will need a nice picture, perhaps a pagewide one with labels of data for 1 trial?

\subsection{Preprocessing}

\subsection{Dimensionality Reduction}
%Useful background info: http://www.brainvoyager.com/bvqx/doc/UsersGuide/WebHelp/Content/MVPATools/MVPA_Basic_Concepts.htm
Typical input for the machine learning algorithms discussed in this paper consists of a number of feature vectors, one for each fMRI scan. Such a feature vector $\vec{x}$ contains measures for the $N$ features we use to represent that scan and is derived from our input data. In the most basic case, we could use the entire set of voxels belonging to a scan, resulting in a feature vector containing potentially (hundreds of) thousands of features.
In the case that we have a very large dimension of features combined with only a limited amount of scans, our classifier may have difficulty learning the right classification function. This problem is formally known as the \textit{Hughes effect}\cite{Hughes1054102} which states that the predictive power of a machine learning algorithm decreases as the dimensionality increases with a fixed number of training examples.

To reducde the dimensionality of the input, two different methods are used.

ROI mapping. ROI mapping is a form of feature selection, wherein only a subset of the d-dimensional input space is used. As described in the introduction, ROIs are anatomically defined brain areas classified as relevant by an expert in the field. To reduce the number of features, average voxel values within this regions are taken as input features. As other higher-order features such as the standard deviation, kurtosis and skewness within ROIs did not increase predictability, these were not included in the subsequent analysis.

Principal Component Analysis (PCA). As feature extraction method, we applied PCA. In between subject classification, the input vectors of the PCA should ideally be the standardized Talairach coordinates. As this coordinate system appeared to be inaccurate for this dataset, we were forced to use the voxel indices. Because brain sizes and shapes differ per subject, it is uncertain whether the voxel indices map well onto each other. As a result, the accuracy of the PCA is reduced.   


So how do we cope with this problem? On common solution is to reduce $|\vec{x}|$ by only using voxels belonging to a specific region of interest (ROI). However, this requires us to known which regions are actually of interest \textit{a priori}.
- ROIs, means, stddev, skew, kurtosis?

\subsection{Classifiers}



- same for each algorithm:
    - What features did we select?
    - How did we train?
    - How did we test?
        - How did we calculate performance?
- algorithms:
    - gaussian naive-bayes (gnb)
        - pca + gnb
    - multivariate
    - svm
    - why multivariate is not a priori better than naive: less assumption, higher generalizability, better estimation parameters. 



\section{Experimental Results}
\label{sec:results}

As a comparison measure, we used the performance of the classifiers for predicting the cognitive state that the subject was in. Table~\ref{tab:results} illustrates the overall performance of the three classifiers. Rows indicate various feature selection methods, where \emph{M} stands for \emph{Mean}, \emph{Norm} for \emph{Normalisation}, and \emph{SD} for \emph{Standard Deviation}. For instance, \emph{Norm-M-SD ROIs} denotes that normalisation was used in the voxels belonging to the selected ROIs over the mean and standard deviation.

\begin{table*}[htpb]
\centering
{\small
\begin{tabular}{c|c|c|c|c}
& \textbf{Naive Bayes} & \textbf{Linear SVM} & \textbf{Naive Bayes*} & \textbf{Multivariate*} \\ \hline
PCA & 61.46 &  - & - & - \\
M ROIs & 70.21 & 72.5 & 74.79 & 76.67 \\
Norm-M ROIs & 71.67 & 71.25 & 75.83 & 77.08 \\
Norm-M-SD ROIs & 65.63 & 69.79 & 69.38 & 65.0 \\
Norm/Vox-M ROIs & 73.13 & 76.67 & 76.88 & 79.58 \\
\end{tabular}}
\caption{Performance results (in \%) of the three classifiers on different methods for feature selection and structure.}
\label{tab:results}
\end{table*}

Columns depict the used classifiers---Naive Bayes, Linear SVM, and Multivariate. For the classifiers depicted with *, we used different structure for the data that we fit to them, as described in Section~\ref{}. The table reveals the following general results:

\begin{itemize}
	\item Classifiers performance do not differ significantly between them. Except from the case that PCA is used, the difference in performance between the classifiers for the rest of the cases is not more than 10\%.
	\item Classifiers performance differ significantly from 50\%.
\end{itemize}

Moreover, it is observed that the Multivariate case has the best performance between. When PCA is used for feature extraction, the performance of Naive Bayes is lower than when other feature selection methods are used. 

We can also distinguish the fact that when different structure is used for the fitting data, the performance increases, though not significantly. For example, in the case of the Naive Bayes classifier when different structure is used, its performance increases approximately 4\%.

Comparing classifiers when the same structure is used, it is observed that Linear SVM performs better than Naive Bayes. The same remains true when comparing the Multivariate case with Naive Bayes. The former one has better performance.

%mention confidence interval (how much percent sure we are that the actual accuracy of the classifier is between these regions)


\section{Discussion / Conclusion}
\label{sec:discussion}

\bibliographystyle{abbrv}
%%use following if all content of bibtex file should be shown
\nocite{*}
\bibliography{references}
\end{document}